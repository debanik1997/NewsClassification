{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Base_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPcgXEWI8ZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c95764ca-308e-4d9d-ca29-039872b61e27"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, NamedTuple\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm, svd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HecDdqTgiIWj",
        "colab_type": "text"
      },
      "source": [
        "File IO and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CiWULfI8Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Document(NamedTuple):\n",
        "    doc_id: str\n",
        "    category: str\n",
        "    link: str\n",
        "    date: str\n",
        "    headline: List[str]\n",
        "    short_description: List[str]\n",
        "\n",
        "    def sections(self):\n",
        "        return [self.headline, self.short_description]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"doc_id: {self.doc_id}\\n\" +\n",
        "            f\"  category: {self.category}\\n\" +\n",
        "            f\"  link: {self.link}\\n\" +\n",
        "            f\"  date: {self.date}\" +\n",
        "            f\"  headline: {self.headline}\" +\n",
        "            f\"  short_description: {self.short_description}\")\n",
        "\n",
        "\n",
        "def read_stopwords(file):\n",
        "    with open(file) as f:\n",
        "        return set([x.strip() for x in f.readlines()])\n",
        "\n",
        "stopwords = read_stopwords('common_words')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "def read_docs(file):\n",
        "    '''\n",
        "    Reads the corpus into a list of Documents\n",
        "    '''\n",
        "    docs = []  # empty 0 index\n",
        "    categories = set()\n",
        "    with open(file) as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            json_dict = json.loads(line)\n",
        "            doc_id = i\n",
        "            category = json_dict['category']\n",
        "            categories.add(category)\n",
        "            link = json_dict['link']\n",
        "            date = json_dict['date']\n",
        "            headline = []\n",
        "            short_description = []\n",
        "\n",
        "            ws = word_tokenize(json_dict['headline'])\n",
        "            for word in ws:\n",
        "                headline.append(word.lower())\n",
        "\n",
        "            ws = word_tokenize(json_dict['short_description'])\n",
        "            for word in ws:\n",
        "                short_description.append(word.lower())\n",
        "            docs.append(Document(doc_id, category, link, date, headline, short_description))\n",
        "            i += 1\n",
        "\n",
        "    return docs, categories\n",
        "\n",
        "\n",
        "def stem_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[stemmer.stem(word) for word in sec]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def stem_docs(docs: List[Document]):\n",
        "    return [stem_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def remove_stopwords_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[word for word in sec if word not in stopwords]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def remove_stopwords(docs: List[Document]):\n",
        "    return [remove_stopwords_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def writelines(filename, data):\n",
        "    with open(filename, 'w') as fout:\n",
        "        for d in data:\n",
        "            print(d, file=fout)\n",
        "\n",
        "\n",
        "def process_docs(training_docs, testing_docs, stem):\n",
        "    processed_traindocs = training_docs\n",
        "    processed_devdocs = testing_docs\n",
        "\n",
        "    if stem:\n",
        "        processed_traindocs = stem_docs(processed_traindocs)\n",
        "        processed_devdocs = stem_docs(processed_devdocs)\n",
        "    return processed_traindocs, processed_devdocs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hagHDXMiUAi",
        "colab_type": "text"
      },
      "source": [
        "Position weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4AVh-DLtOnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TermWeights(NamedTuple):\n",
        "    headline: float\n",
        "    short_description: float"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0DnoPX1ihT1",
        "colab_type": "text"
      },
      "source": [
        "Term-Document Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGIxU6bcieMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_doc_freqs(docs: List[Document]):\n",
        "    '''\n",
        "    Computes document frequency, i.e. how many documents contain a specific word\n",
        "    '''\n",
        "    freq = Counter()\n",
        "    for doc in docs:\n",
        "        words = set()\n",
        "        for sec in doc.sections():\n",
        "            for word in sec:\n",
        "                words.add(word)\n",
        "        for word in words:\n",
        "            freq[word] += 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def compute_tf(doc: Document, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] += weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] += weights.short_description\n",
        "    return dict(vec)  # convert back to a regular dict\n",
        "\n",
        "\n",
        "def compute_tfidf(doc: Document, doc_freqs, weights, N):\n",
        "    tf = compute_tf(doc, doc_freqs, weights, N)\n",
        "    vec = defaultdict(float)\n",
        "    for k, v in tf.items():\n",
        "        if doc_freqs[k] == 0:\n",
        "            continue\n",
        "        vec[k] = v * np.log(float(N) / (doc_freqs[k]))\n",
        "\n",
        "    return dict(vec)  # TODO: implement\n",
        "\n",
        "def compute_boolean(doc, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] = 1 * weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] = 1 * weights.short_description\n",
        "    return dict(vec)  # TODO: implement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYUCh1RsilDY",
        "colab_type": "text"
      },
      "source": [
        "Vector Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7XiYHJ5ims3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dictdot(x: Dict[str, float], y: Dict[str, float]):\n",
        "    '''\n",
        "    Computes the dot product of vectors x and y, represented as sparse dictionaries.\n",
        "    '''\n",
        "    keys = list(x.keys()) if len(x) < len(y) else list(y.keys())\n",
        "    return sum(x.get(key, 0) * y.get(key, 0) for key in keys)\n",
        "\n",
        "\n",
        "def cosine_sim(x, y):\n",
        "    '''\n",
        "    Computes the cosine similarity between two sparse term vectors represented as dictionaries.\n",
        "    '''\n",
        "    num = dictdot(x, y)\n",
        "    if num == 0:\n",
        "        return 0\n",
        "    return num / (norm(list(x.values())) * norm(list(y.values())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsjiM1Yi_OK",
        "colab_type": "text"
      },
      "source": [
        "Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoQrZvli-nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = 'News_Category_Dataset.json'\n",
        "\n",
        "term_funcs = {\n",
        "    'tfidf': compute_tfidf,\n",
        "    'tf': compute_tf,\n",
        "    'boolean': compute_boolean\n",
        "}\n",
        "\n",
        "sim_funcs = {\n",
        "    'cosine': cosine_sim\n",
        "}\n",
        "\n",
        "region_weights = {\n",
        "    0: TermWeights(headline=1, short_description=1),\n",
        "    1: TermWeights(headline=3, short_description=1),\n",
        "    2: TermWeights(headline=1, short_description=3)\n",
        "}\n",
        "\n",
        "permutations = [\n",
        "    term_funcs,\n",
        "    [False, True],  # stem\n",
        "    sim_funcs,\n",
        "    region_weights\n",
        "]\n",
        "\n",
        "# permutations = [\n",
        "#     ('tfidf', False, 'cosine', 0)\n",
        "# ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahlBRPt5FGky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sep_docs(file):\n",
        "    docs = defaultdict(list)\n",
        "    for d in file:\n",
        "        docs[d.category].append(d)\n",
        "    return docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp2JDpCSE_vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, categories = read_docs(file)\n",
        "sep_doc = sep_docs(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QykHZjr5jGPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "10e20ad0-f1c5-4c81-ad75-5fb0e35718f8"
      },
      "source": [
        "training_docs = []\n",
        "testing_docs = []\n",
        "for c in categories:\n",
        "    print(c, len(sep_doc[c]))\n",
        "    split = int(len(sep_doc[c]) * 0.9)\n",
        "    training_docs += (sep_doc[c][:split])\n",
        "    testing_docs += (sep_doc[c][split:])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TASTE 2096\n",
            "HEALTHY LIVING 6694\n",
            "WELLNESS 17827\n",
            "STYLE & BEAUTY 9649\n",
            "LATINO VOICES 1129\n",
            "WORLD NEWS 2177\n",
            "QUEER VOICES 6314\n",
            "THE WORLDPOST 3664\n",
            "COMEDY 5175\n",
            "COLLEGE 1144\n",
            "ENVIRONMENT 1323\n",
            "STYLE 2254\n",
            "GOOD NEWS 1398\n",
            "HOME & LIVING 4195\n",
            "MEDIA 2815\n",
            "CULTURE & ARTS 1030\n",
            "WOMEN 3490\n",
            "PARENTING 8677\n",
            "GREEN 2622\n",
            "ENTERTAINMENT 16058\n",
            "TECH 2082\n",
            "FIFTY 1401\n",
            "RELIGION 2556\n",
            "IMPACT 3459\n",
            "CRIME 3405\n",
            "ARTS 1509\n",
            "WEIRD NEWS 2670\n",
            "DIVORCE 3426\n",
            "TRAVEL 9887\n",
            "WEDDINGS 3651\n",
            "SCIENCE 2178\n",
            "FOOD & DRINK 6226\n",
            "PARENTS 3955\n",
            "BLACK VOICES 4528\n",
            "WORLDPOST 2579\n",
            "BUSINESS 5937\n",
            "POLITICS 32739\n",
            "ARTS & CULTURE 1339\n",
            "MONEY 1707\n",
            "EDUCATION 1004\n",
            "SPORTS 4884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ati5_8pOJ8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d713a86-09da-4b85-8588-d8fc9866403f"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_docs)\n",
        "random.shuffle(testing_docs)\n",
        "print(\"Training data\", len(training_docs))\n",
        "print(\"Testing data\",len(testing_docs))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data 180752\n",
            "Testing data 20101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIhL1hWjjr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "8eea0f14-4911-49f5-d6d4-7611552c8c2f"
      },
      "source": [
        "print('category', 'accuracy', 'precision', 'recall', 'F1_Score', sep='\\t')\n",
        "\n",
        "for term, stem, sim, weight in itertools.product(*permutations):\n",
        "\n",
        "    processed_traindocs, processed_devdocs = process_docs(training_docs, testing_docs, stem)\n",
        "    doc_freqs_train = compute_doc_freqs(processed_traindocs)\n",
        "    doc_freqs_test = compute_doc_freqs(processed_devdocs)\n",
        "    N_train = len(processed_traindocs)\n",
        "    N_test = len(processed_devdocs)\n",
        "    metrics = []\n",
        "\n",
        "    # create Vprofile\n",
        "    vprofile = defaultdict(lambda: defaultdict(float))\n",
        "    train_vector_sense = defaultdict(list)\n",
        "\n",
        "    for train_doc in processed_traindocs:\n",
        "        train_vector = term_funcs[term](train_doc, doc_freqs_train, region_weights[weight], N_train)\n",
        "        train_vector_sense[train_doc.category].append(train_vector)\n",
        "\n",
        "    # do centriod\n",
        "    for category, vecs in train_vector_sense.items():\n",
        "        for doc in vecs:\n",
        "            for word in doc:\n",
        "                vprofile[category][word] += doc[word]\n",
        "\n",
        "    for category, vec in vprofile.items():\n",
        "        for word in vec:\n",
        "            vec[word] /= len(vec)\n",
        "\n",
        "    # calculate correctness\n",
        "    correct_count = 0\n",
        "    sims = defaultdict(float)\n",
        "    tp = defaultdict(float)\n",
        "    fn = defaultdict(float)\n",
        "    fp = defaultdict(float)\n",
        "\n",
        "    for test_doc in processed_devdocs:\n",
        "        test_vector = term_funcs[term](test_doc, doc_freqs_test, region_weights[weight], N_test)\n",
        "        for category, vec in vprofile.items():\n",
        "            sims[category] = sim_funcs[sim](test_vector, vec)\n",
        "\n",
        "        predict = max(sims, key=lambda key: sims[key])\n",
        "        if predict == test_doc.category:\n",
        "            correct_count += 1\n",
        "            tp[predict] += 1\n",
        "        else:\n",
        "            fp[predict] += 1\n",
        "            fn[test_doc.category] += 1\n",
        "\n",
        "        metrics.append([\n",
        "            test_doc.doc_id, predict, test_doc.category\n",
        "        ])\n",
        "\n",
        "    total_acc = correct_count / len(processed_devdocs)\n",
        "    accuracy = defaultdict(float)\n",
        "    precison = defaultdict(float)\n",
        "    recall = defaultdict(float)\n",
        "    f1_score = defaultdict(float)\n",
        "\n",
        "    for category in categories:\n",
        "        accuracy[category] = tp[category] / (tp[category] + fp[category]) if (tp[category] + fp[category]) != 0 else 0\n",
        "        precison[category] = tp[category] / (tp[category] + fn[category]) if (tp[category] + fn[category]) != 0 else 0\n",
        "        recall[category] = (tp[category] + N_test - fn[category]) / (tp[category] + fp[category] + N_test)\n",
        "        f1_score[category] = 2 * precison[category] * recall[category] / (precison[category] + recall[category])\n",
        "        print(category, accuracy[category], precison[category], recall[category], f1_score[category], sep='\\t')\n",
        "\n",
        "    print(term, stem, sim, weight, str(total_acc))\n",
        "\n",
        "    break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category\taccuracy\tprecision\trecall\tF1_Score\n",
            "TASTE\t0.21329639889196675\t0.36666666666666664\t0.9796207604339752\t0.5336071207310462\n",
            "HEALTHY LIVING\t0.18547595682041218\t0.282089552238806\t0.9379261363636363\t0.43373075659866855\n",
            "WELLNESS\t0.544189852700491\t0.37296690970274815\t0.921446325563945\t0.5310035143942722\n",
            "STYLE & BEAUTY\t0.5722379603399433\t0.627979274611399\t0.9616257088846881\t0.7597875212808368\n",
            "LATINO VOICES\t0.3655913978494624\t0.3008849557522124\t0.9931662870159453\t0.46185001713555796\n",
            "WORLD NEWS\t0.16923076923076924\t0.15137614678899083\t0.9829030350808041\t0.2623482410608431\n",
            "QUEER VOICES\t0.7753623188405797\t0.5079113924050633\t0.9803070923714355\t0.6691344656234546\n",
            "THE WORLDPOST\t0.41586538461538464\t0.4713896457765668\t0.9787005897548374\t0.6363042975139852\n",
            "COMEDY\t0.5568513119533528\t0.3687258687258687\t0.976570142829192\t0.5353270524755461\n",
            "COLLEGE\t0.2524752475247525\t0.4434782608695652\t0.9894104319558686\t0.6124439669975816\n",
            "ENVIRONMENT\t0.16929133858267717\t0.3233082706766917\t0.9852124785065095\t0.48685103827947185\n",
            "STYLE\t0.24202127659574468\t0.4026548672566372\t0.9794891829857889\t0.5707018554039694\n",
            "GOOD NEWS\t0.14814814814814814\t0.2571428571428571\t0.9847129374754228\t0.4077959764817908\n",
            "HOME & LIVING\t0.5224489795918368\t0.6095238095238096\t0.9806711670147151\t0.7517850759535313\n",
            "MEDIA\t0.37971014492753624\t0.4645390070921986\t0.9821480974273696\t0.6307460688231094\n",
            "CULTURE & ARTS\t0.2644230769230769\t0.5339805825242718\t0.9901029100398838\t0.6937884062671436\n",
            "WOMEN\t0.2386634844868735\t0.28653295128939826\t0.9723196881091618\t0.4426278678079627\n",
            "PARENTING\t0.49523809523809526\t0.2995391705069124\t0.957674779404635\t0.456344139449673\n",
            "GREEN\t0.33695652173913043\t0.35361216730038025\t0.9826765470874025\t0.5200768065006529\n",
            "ENTERTAINMENT\t0.6201388888888889\t0.5560398505603985\t0.94150689383037\t0.6991639553261496\n",
            "TECH\t0.5061224489795918\t0.5933014354066986\t0.9898751597365576\t0.7419189431511176\n",
            "FIFTY\t0.046374367622259695\t0.3900709219858156\t0.9428289566402029\t0.5518346370781785\n",
            "RELIGION\t0.46963562753036436\t0.453125\t0.9866817377629251\t0.6210419088863052\n",
            "IMPACT\t0.1716514954486346\t0.3815028901734104\t0.959223766171538\t0.5458929862931179\n",
            "CRIME\t0.3333333333333333\t0.5630498533724341\t0.9742225661362867\t0.7136482331354814\n",
            "ARTS\t0.16666666666666666\t0.33112582781456956\t0.982794961031322\t0.4953552722601048\n",
            "WEIRD NEWS\t0.2222222222222222\t0.2397003745318352\t0.9790573348374123\t0.3851141339161235\n",
            "DIVORCE\t0.8837209302325582\t0.6647230320699709\t0.99287784272312\t0.7963180764759028\n",
            "TRAVEL\t0.7159763313609467\t0.48938321536905965\t0.9664532896953362\t0.6497515576368766\n",
            "WEDDINGS\t0.7824427480916031\t0.5601092896174863\t0.9892943083042773\t0.7152596431170943\n",
            "SCIENCE\t0.404320987654321\t0.6009174311926605\t0.9862913096695226\t0.7468200306058125\n",
            "FOOD & DRINK\t0.5967153284671532\t0.5248796147672552\t0.9749624679161218\t0.6823890734640448\n",
            "PARENTS\t0.25552825552825553\t0.26262626262626265\t0.9709869319290033\t0.4134305147139558\n",
            "BLACK VOICES\t0.3602693602693603\t0.23620309050772628\t0.9737229140111776\t0.38018252476370457\n",
            "WORLDPOST\t0.10467980295566502\t0.32945736434108525\t0.9569645674939034\t0.4901642554004425\n",
            "BUSINESS\t0.35398230088495575\t0.3367003367003367\t0.9632730088067357\t0.49898614847839556\n",
            "POLITICS\t0.8094804010938924\t0.2712278558338424\t0.8775827908293236\t0.4143849108026352\n",
            "ARTS & CULTURE\t0.2236842105263158\t0.2537313432835821\t0.9892361625438206\t0.40387253756864216\n",
            "MONEY\t0.24561403508771928\t0.5730994152046783\t0.9817560975609756\t0.7237249259065055\n",
            "EDUCATION\t0.23938223938223938\t0.6138613861386139\t0.9884086444007859\t0.7573578597347856\n",
            "SPORTS\t0.5692883895131086\t0.621676891615542\t0.9798885388902351\t0.7607232890817308\n",
            "tfidf True cosine 0 0.4123177951345704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfnmZ76oFZ8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4b9a84a8-4151-4af0-8119-7ca2fa5fa772"
      },
      "source": [
        "selected_categories = ['QUEER VOICES', 'SPORTS', 'COMEDY', 'HEALTHY LIVING', 'TRAVEL']\n",
        "training_docs = []\n",
        "testing_docs = []\n",
        "for c in selected_categories:\n",
        "    print(c, len(sep_doc[c]))\n",
        "    split = int(len(sep_doc[c]) * 0.9)\n",
        "    training_docs += (sep_doc[c][:split])\n",
        "    testing_docs += (sep_doc[c][split:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUEER VOICES 6314\n",
            "SPORTS 4884\n",
            "COMEDY 5175\n",
            "HEALTHY LIVING 6694\n",
            "TRAVEL 9887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXLiEeaFJcJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ffacdcc-8aaa-43be-a78b-c4315d9156a3"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_docs)\n",
        "random.shuffle(testing_docs)\n",
        "print(\"Training data\", len(training_docs))\n",
        "print(\"Testing data\",len(testing_docs))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data 29656\n",
            "Testing data 3298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xbe0KP6FkBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c7d9a91-81d8-4c74-f7f6-e9fb5d389c74"
      },
      "source": [
        "print('category', 'accuracy', 'precision', 'recall', 'F1_Score', sep='\\t')\n",
        "\n",
        "for term, stem, sim, weight in itertools.product(*permutations):\n",
        "\n",
        "    processed_traindocs, processed_devdocs = process_docs(training_docs, testing_docs, stem)\n",
        "    doc_freqs_train = compute_doc_freqs(processed_traindocs)\n",
        "    doc_freqs_test = compute_doc_freqs(processed_devdocs)\n",
        "    N_train = len(processed_traindocs)\n",
        "    N_test = len(processed_devdocs)\n",
        "    metrics = []\n",
        "\n",
        "    # create Vprofile\n",
        "    vprofile = defaultdict(lambda: defaultdict(float))\n",
        "    train_vector_sense = defaultdict(list)\n",
        "\n",
        "    for train_doc in processed_traindocs:\n",
        "        train_vector = term_funcs[term](train_doc, doc_freqs_train, region_weights[weight], N_train)\n",
        "        train_vector_sense[train_doc.category].append(train_vector)\n",
        "\n",
        "    # do centriod\n",
        "    for category, vecs in train_vector_sense.items():\n",
        "        for doc in vecs:\n",
        "            for word in doc:\n",
        "                vprofile[category][word] += doc[word]\n",
        "\n",
        "    for category, vec in vprofile.items():\n",
        "        for word in vec:\n",
        "            vec[word] /= len(vec)\n",
        "\n",
        "    # calculate correctness\n",
        "    correct_count = 0\n",
        "    sims = defaultdict(float)\n",
        "    tp = defaultdict(float)\n",
        "    fn = defaultdict(float)\n",
        "    fp = defaultdict(float)\n",
        "\n",
        "    for test_doc in processed_devdocs:\n",
        "        test_vector = term_funcs[term](test_doc, doc_freqs_test, region_weights[weight], N_test)\n",
        "        for category, vec in vprofile.items():\n",
        "            sims[category] = sim_funcs[sim](test_vector, vec)\n",
        "\n",
        "        predict = max(sims, key=lambda key: sims[key])\n",
        "        if predict == test_doc.category:\n",
        "            correct_count += 1\n",
        "            tp[predict] += 1\n",
        "        else:\n",
        "            fp[predict] += 1\n",
        "            fn[test_doc.category] += 1\n",
        "\n",
        "        metrics.append([\n",
        "            test_doc.doc_id, predict, test_doc.category\n",
        "        ])\n",
        "\n",
        "    total_acc = correct_count / len(processed_devdocs)\n",
        "    accuracy = defaultdict(float)\n",
        "    precison = defaultdict(float)\n",
        "    recall = defaultdict(float)\n",
        "    f1_score = defaultdict(float)\n",
        "\n",
        "    for category in selected_categories:\n",
        "        accuracy[category] = tp[category] / (tp[category] + fp[category]) if (tp[category] + fp[category]) != 0 else 0\n",
        "        precison[category] = tp[category] / (tp[category] + fn[category]) if (tp[category] + fn[category]) != 0 else 0\n",
        "        recall[category] = (tp[category] + N_test - fn[category]) / (tp[category] + fp[category] + N_test)\n",
        "        f1_score[category] = 2 * precison[category] * recall[category] / (precison[category] + recall[category])\n",
        "        print(category, accuracy[category], precison[category], recall[category], f1_score[category], sep='\\t')\n",
        "\n",
        "    print(term, stem, sim, region_weights[weight], str(total_acc))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category\taccuracy\tprecision\trecall\tF1_Score\n",
            "QUEER VOICES\t0.8393782383419689\t0.7689873417721519\t0.9383543977302038\t0.8452703255073936\n",
            "SPORTS\t0.8123791102514507\t0.8588957055214724\t0.9564875491480996\t0.9050684435200336\n",
            "COMEDY\t0.7901234567901234\t0.6177606177606177\t0.9235754793410748\t0.7403298472528512\n",
            "HEALTHY LIVING\t0.7613168724279835\t0.8283582089552238\t0.9282344176806556\t0.8754569364134407\n",
            "TRAVEL\t0.7921348314606742\t0.8554095045500506\t0.9163994502977554\t0.8848547667674209\n",
            "tfidf True cosine TermWeights(headline=1, short_description=1) 0.7965433596118859\n",
            "QUEER VOICES\t0.8862385321100917\t0.7642405063291139\t0.9450949778818631\t0.8451001820265137\n",
            "SPORTS\t0.8501026694045175\t0.8466257668711656\t0.9608982826948481\t0.9001498438342086\n",
            "COMEDY\t0.7967032967032966\t0.5598455598455598\t0.9175314036045876\t0.6953890510480077\n",
            "HEALTHY LIVING\t0.7378516624040921\t0.8611940298507462\t0.9269607843137255\t0.8928679855158446\n",
            "TRAVEL\t0.7589285714285714\t0.8594539939332659\t0.9074241738343142\t0.8827878963254973\n",
            "tfidf True cosine TermWeights(headline=3, short_description=1) 0.7926015767131595\n",
            "QUEER VOICES\t0.7665505226480837\t0.6962025316455697\t0.9158057851239669\t0.7910459263345325\n",
            "SPORTS\t0.7387033398821218\t0.7689161554192229\t0.9353821907013397\t0.8440194518276039\n",
            "COMEDY\t0.6130434782608696\t0.5444015444015444\t0.8898350186269293\t0.6755197446370778\n",
            "HEALTHY LIVING\t0.6974900924702774\t0.7880597014925373\t0.9085080147965475\t0.8440082268100284\n",
            "TRAVEL\t0.7625250501002004\t0.7694641051567239\t0.8917597765363129\t0.8261103708284719\n",
            "tfidf True cosine TermWeights(headline=1, short_description=3) 0.7237719830200121\n",
            "QUEER VOICES\t0.8155172413793104\t0.7484177215189873\t0.9314079422382672\t0.8299458985231759\n",
            "SPORTS\t0.8107569721115537\t0.8323108384458078\t0.953421052631579\t0.8887590345143084\n",
            "COMEDY\t0.7737789203084833\t0.581081081081081\t0.9172769189042582\t0.7114618318089865\n",
            "HEALTHY LIVING\t0.7395411605937922\t0.817910447761194\t0.9220103986135182\t0.8668462586004912\n",
            "TRAVEL\t0.7670349907918969\t0.8422649140546006\t0.9067062043795621\t0.87329838126572\n",
            "tfidf False cosine TermWeights(headline=1, short_description=1) 0.7768344451182535\n",
            "QUEER VOICES\t0.8809073724007561\t0.7373417721518988\t0.9401620067938333\t0.82649080008025\n",
            "SPORTS\t0.8631578947368421\t0.8384458077709611\t0.9618340842830638\t0.8959115294213988\n",
            "COMEDY\t0.7612359550561798\t0.5231660231660231\t0.9091406677613574\t0.6641475749069542\n",
            "HEALTHY LIVING\t0.7276214833759591\t0.8492537313432836\t0.9230392156862746\t0.884610526055062\n",
            "TRAVEL\t0.7344290657439446\t0.8584428715874621\t0.8996407723394702\t0.8785591182444948\n",
            "tfidf False cosine TermWeights(headline=3, short_description=1) 0.7777440873256519\n",
            "QUEER VOICES\t0.7575221238938054\t0.6772151898734177\t0.9117266373285011\t0.7771651765229934\n",
            "SPORTS\t0.7117988394584139\t0.7525562372188139\t0.9292267365661862\t0.8316119110416983\n",
            "COMEDY\t0.5986547085201793\t0.5154440154440154\t0.8851495726495726\t0.6515024113684055\n",
            "HEALTHY LIVING\t0.6803170409511229\t0.7686567164179104\t0.9020961775585696\t0.8300476847716054\n",
            "TRAVEL\t0.7374136229022705\t0.7553083923154702\t0.8821619113894688\t0.813821531353506\n",
            "tfidf False cosine TermWeights(headline=1, short_description=3) 0.7049727107337781\n",
            "QUEER VOICES\t0.7073791348600509\t0.439873417721519\t0.8729341641831482\t0.5849761069905257\n",
            "SPORTS\t0.3617886178861789\t0.36400817995910023\t0.8350923482849604\t0.5070141137242639\n",
            "COMEDY\t0.5316091954022989\t0.35714285714285715\t0.863960504662644\t0.5053746189635809\n",
            "HEALTHY LIVING\t0.5991316931982634\t0.6179104477611941\t0.8663825520180496\t0.7213492628875569\n",
            "TRAVEL\t0.4781659388646288\t0.6643073811931244\t0.775470890410959\t0.7155977369021764\n",
            "tf True cosine TermWeights(headline=1, short_description=1) 0.5191024863553669\n",
            "QUEER VOICES\t0.7828162291169452\t0.5189873417721519\t0.8937315039009954\t0.6566562609230175\n",
            "SPORTS\t0.4386503067484663\t0.29243353783231085\t0.8540286975717439\t0.43568226798714565\n",
            "COMEDY\t0.5872093023255814\t0.38996138996138996\t0.8742449203734212\t0.5393451393629172\n",
            "HEALTHY LIVING\t0.6454948301329394\t0.6522388059701493\t0.8810062893081761\t0.7495559476565571\n",
            "TRAVEL\t0.48629242819843344\t0.7532861476238625\t0.7865424430641822\t0.7695551705709461\n",
            "tf True cosine TermWeights(headline=3, short_description=1) 0.5624620982413584\n",
            "QUEER VOICES\t0.5621301775147929\t0.30063291139240506\t0.8377337733773378\t0.44247665823620974\n",
            "SPORTS\t0.30256410256410254\t0.3619631901840491\t0.814576358485707\t0.5012099384153433\n",
            "COMEDY\t0.4146341463414634\t0.2625482625482625\t0.8416988416988417\t0.4002484002484002\n",
            "HEALTHY LIVING\t0.5079365079365079\t0.573134328358209\t0.8376911692155895\t0.6806080078161697\n",
            "TRAVEL\t0.4058869093725794\t0.5298281092012134\t0.731531924166485\t0.6145528095838584\n",
            "tf True cosine TermWeights(headline=1, short_description=3) 0.42783505154639173\n",
            "QUEER VOICES\t0.6675824175824175\t0.38449367088607594\t0.8607318405243036\t0.5315437918339445\n",
            "SPORTS\t0.3240556660039761\t0.3333333333333333\t0.824782951854775\t0.47478418900499764\n",
            "COMEDY\t0.5172413793103449\t0.3474903474903475\t0.8612177729018102\t0.49518135622919224\n",
            "HEALTHY LIVING\t0.5874635568513119\t0.6014925373134329\t0.8619477911646586\t0.708542950266388\n",
            "TRAVEL\t0.45526127415891193\t0.6430738119312437\t0.7627263045793398\t0.6978080402547387\n",
            "tf False cosine TermWeights(headline=1, short_description=1) 0.49272286234081264\n",
            "QUEER VOICES\t0.7551020408163265\t0.46835443037974683\t0.8829268292682927\t0.6120453299214038\n",
            "SPORTS\t0.375\t0.25153374233128833\t0.8425261996690568\t0.38740796528468324\n",
            "COMEDY\t0.5461956521739131\t0.38803088803088803\t0.8679759956355702\t0.5363051759601735\n",
            "HEALTHY LIVING\t0.6295180722891566\t0.6238805970149254\t0.8743059061080263\t0.7281636692619253\n",
            "TRAVEL\t0.4663648124191462\t0.7290192113245703\t0.7743600330305532\t0.7510058991180967\n",
            "tf False cosine TermWeights(headline=3, short_description=1) 0.5333535476046088\n",
            "QUEER VOICES\t0.5235109717868338\t0.26424050632911394\t0.8294166436273155\t0.4007937476175372\n",
            "SPORTS\t0.2866666666666667\t0.35173824130879344\t0.8088763468445357\t0.49027945466043293\n",
            "COMEDY\t0.40425531914893614\t0.25675675675675674\t0.8398125172318721\t0.39327663709537675\n",
            "HEALTHY LIVING\t0.49800796812749004\t0.5597014925373134\t0.8338681806961245\t0.6698154735702474\n",
            "TRAVEL\t0.3855050115651503\t0.5055611729019212\t0.7201305767138194\t0.5940646318621512\n",
            "tf False cosine TermWeights(headline=1, short_description=3) 0.40842935112189205\n",
            "QUEER VOICES\t0.7768924302788844\t0.6170886075949367\t0.9068421052631579\t0.7344191272263073\n",
            "SPORTS\t0.5899280575539568\t0.33537832310838445\t0.8772371364653244\t0.48524256799359766\n",
            "COMEDY\t0.5552238805970149\t0.3590733590733591\t0.8676025323424167\t0.507930347060439\n",
            "HEALTHY LIVING\t0.6720116618075802\t0.6880597014925374\t0.8910642570281124\t0.7765133362624682\n",
            "TRAVEL\t0.5156980627922512\t0.7805864509605662\t0.8035453597497393\t0.7918995329961023\n",
            "boolean True cosine TermWeights(headline=1, short_description=1) 0.5982413583990297\n",
            "QUEER VOICES\t0.8032407407407407\t0.5490506329113924\t0.900804289544236\t0.6822574557540896\n",
            "SPORTS\t0.5015015015015015\t0.34151329243353784\t0.8656017625998348\t0.4897867964600448\n",
            "COMEDY\t0.5140845070422535\t0.28185328185328185\t0.8576214405360134\t0.42427166281663437\n",
            "HEALTHY LIVING\t0.6242038216560509\t0.7313432835820896\t0.8836639725691893\t0.8003205048403927\n",
            "TRAVEL\t0.48360655737704916\t0.7158746208291203\t0.7822343553128938\t0.747584763761599\n",
            "boolean True cosine TermWeights(headline=3, short_description=1) 0.5633717404487568\n",
            "QUEER VOICES\t0.6750572082379863\t0.46677215189873417\t0.8717536813922356\t0.6079977414984158\n",
            "SPORTS\t0.44984802431610943\t0.30265848670756645\t0.8560794044665012\t0.44721019141752316\n",
            "COMEDY\t0.41297935103244837\t0.2702702702702703\t0.8413527632664284\t0.4091182565679524\n",
            "HEALTHY LIVING\t0.6002844950213371\t0.6298507462686567\t0.8677830542364409\t0.7299164911018661\n",
            "TRAVEL\t0.45503355704697984\t0.6855409504550051\t0.7654553049289892\t0.7232974658959046\n",
            "boolean True cosine TermWeights(headline=1, short_description=3) 0.5103092783505154\n",
            "QUEER VOICES\t0.7459349593495935\t0.5806962025316456\t0.8970976253298153\t0.7050255245456574\n",
            "SPORTS\t0.5311355311355311\t0.2965235173824131\t0.8678241388966677\t0.442016204949228\n",
            "COMEDY\t0.5267857142857143\t0.3416988416988417\t0.8624105668684645\t0.48946497663932803\n",
            "HEALTHY LIVING\t0.6681614349775785\t0.6671641791044776\t0.8878245525586085\t0.7618379821480289\n",
            "TRAVEL\t0.49607329842931935\t0.7664307381193124\t0.7925818483215914\t0.7792869619043485\n",
            "boolean False cosine TermWeights(headline=1, short_description=1) 0.5742874469375379\n",
            "QUEER VOICES\t0.8078817733990148\t0.5189873417721519\t0.8968682505399568\t0.6575010499587257\n",
            "SPORTS\t0.4491525423728814\t0.32515337423312884\t0.8562431544359255\t0.4713241389705329\n",
            "COMEDY\t0.5121107266435986\t0.2857142857142857\t0.8575411207136883\t0.4286211941754337\n",
            "HEALTHY LIVING\t0.6119592875318066\t0.7179104477611941\t0.8790401567091087\t0.7903464399419274\n",
            "TRAVEL\t0.4764183185235817\t0.704752275025278\t0.7777777777777778\t0.7394665050015157\n",
            "boolean False cosine TermWeights(headline=3, short_description=1) 0.5497271073377805\n",
            "QUEER VOICES\t0.6699507389162561\t0.43037974683544306\t0.8666306695464363\t0.5751384621870439\n",
            "SPORTS\t0.411214953271028\t0.26993865030674846\t0.8491295938104448\t0.4096495413816871\n",
            "COMEDY\t0.41369047619047616\t0.26833976833976836\t0.8414969730324711\t0.40691949434464403\n",
            "HEALTHY LIVING\t0.5793304221251819\t0.5940298507462687\t0.8592220828105396\t0.7024295702956875\n",
            "TRAVEL\t0.42506459948320413\t0.6653185035389282\t0.7480396203054065\t0.7042583084543622\n",
            "boolean False cosine TermWeights(headline=1, short_description=3) 0.4848392965433596\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}