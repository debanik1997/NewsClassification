{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Base_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPcgXEWI8ZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c95764ca-308e-4d9d-ca29-039872b61e27"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, NamedTuple\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm, svd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HecDdqTgiIWj",
        "colab_type": "text"
      },
      "source": [
        "File IO and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CiWULfI8Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Document(NamedTuple):\n",
        "    doc_id: str\n",
        "    category: str\n",
        "    link: str\n",
        "    date: str\n",
        "    headline: List[str]\n",
        "    short_description: List[str]\n",
        "\n",
        "    def sections(self):\n",
        "        return [self.headline, self.short_description]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"doc_id: {self.doc_id}\\n\" +\n",
        "            f\"  category: {self.category}\\n\" +\n",
        "            f\"  link: {self.link}\\n\" +\n",
        "            f\"  date: {self.date}\" +\n",
        "            f\"  headline: {self.headline}\" +\n",
        "            f\"  short_description: {self.short_description}\")\n",
        "\n",
        "\n",
        "def read_stopwords(file):\n",
        "    with open(file) as f:\n",
        "        return set([x.strip() for x in f.readlines()])\n",
        "\n",
        "stopwords = read_stopwords('common_words')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "def read_docs(file):\n",
        "    '''\n",
        "    Reads the corpus into a list of Documents\n",
        "    '''\n",
        "    docs = []  # empty 0 index\n",
        "    categories = set()\n",
        "    with open(file) as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            json_dict = json.loads(line)\n",
        "            doc_id = i\n",
        "            category = json_dict['category']\n",
        "            categories.add(category)\n",
        "            link = json_dict['link']\n",
        "            date = json_dict['date']\n",
        "            headline = []\n",
        "            short_description = []\n",
        "\n",
        "            ws = word_tokenize(json_dict['headline'])\n",
        "            for word in ws:\n",
        "                headline.append(word.lower())\n",
        "\n",
        "            ws = word_tokenize(json_dict['short_description'])\n",
        "            for word in ws:\n",
        "                short_description.append(word.lower())\n",
        "            docs.append(Document(doc_id, category, link, date, headline, short_description))\n",
        "            i += 1\n",
        "\n",
        "    return docs, categories\n",
        "\n",
        "\n",
        "def stem_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[stemmer.stem(word) for word in sec]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def stem_docs(docs: List[Document]):\n",
        "    return [stem_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def remove_stopwords_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[word for word in sec if word not in stopwords]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def remove_stopwords(docs: List[Document]):\n",
        "    return [remove_stopwords_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def writelines(filename, data):\n",
        "    with open(filename, 'w') as fout:\n",
        "        for d in data:\n",
        "            print(d, file=fout)\n",
        "\n",
        "\n",
        "def process_docs(training_docs, testing_docs, stem):\n",
        "    processed_traindocs = training_docs\n",
        "    processed_devdocs = testing_docs\n",
        "\n",
        "    if stem:\n",
        "        processed_traindocs = stem_docs(processed_traindocs)\n",
        "        processed_devdocs = stem_docs(processed_devdocs)\n",
        "    return processed_traindocs, processed_devdocs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hagHDXMiUAi",
        "colab_type": "text"
      },
      "source": [
        "Position weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4AVh-DLtOnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TermWeights(NamedTuple):\n",
        "    headline: float\n",
        "    short_description: float"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0DnoPX1ihT1",
        "colab_type": "text"
      },
      "source": [
        "Term-Document Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGIxU6bcieMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_doc_freqs(docs: List[Document]):\n",
        "    '''\n",
        "    Computes document frequency, i.e. how many documents contain a specific word\n",
        "    '''\n",
        "    freq = Counter()\n",
        "    for doc in docs:\n",
        "        words = set()\n",
        "        for sec in doc.sections():\n",
        "            for word in sec:\n",
        "                words.add(word)\n",
        "        for word in words:\n",
        "            freq[word] += 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def compute_tf(doc: Document, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] += weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] += weights.short_description\n",
        "    return dict(vec)  # convert back to a regular dict\n",
        "\n",
        "\n",
        "def compute_tfidf(doc: Document, doc_freqs, weights, N):\n",
        "    tf = compute_tf(doc, doc_freqs, weights, N)\n",
        "    vec = defaultdict(float)\n",
        "    for k, v in tf.items():\n",
        "        if doc_freqs[k] == 0:\n",
        "            continue\n",
        "        vec[k] = v * np.log(float(N) / (doc_freqs[k]))\n",
        "\n",
        "    return dict(vec)  # TODO: implement\n",
        "\n",
        "def compute_boolean(doc, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] = 1 * weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] = 1 * weights.short_description\n",
        "    return dict(vec)  # TODO: implement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYUCh1RsilDY",
        "colab_type": "text"
      },
      "source": [
        "Vector Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7XiYHJ5ims3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dictdot(x: Dict[str, float], y: Dict[str, float]):\n",
        "    '''\n",
        "    Computes the dot product of vectors x and y, represented as sparse dictionaries.\n",
        "    '''\n",
        "    keys = list(x.keys()) if len(x) < len(y) else list(y.keys())\n",
        "    return sum(x.get(key, 0) * y.get(key, 0) for key in keys)\n",
        "\n",
        "\n",
        "def cosine_sim(x, y):\n",
        "    '''\n",
        "    Computes the cosine similarity between two sparse term vectors represented as dictionaries.\n",
        "    '''\n",
        "    num = dictdot(x, y)\n",
        "    if num == 0:\n",
        "        return 0\n",
        "    return num / (norm(list(x.values())) * norm(list(y.values())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsjiM1Yi_OK",
        "colab_type": "text"
      },
      "source": [
        "Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoQrZvli-nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = 'News_Category_Dataset.json'\n",
        "\n",
        "term_funcs = {\n",
        "    'tfidf': compute_tfidf,\n",
        "    'tf': compute_tf,\n",
        "    'boolean': compute_boolean\n",
        "}\n",
        "\n",
        "sim_funcs = {\n",
        "    'cosine': cosine_sim\n",
        "}\n",
        "\n",
        "region_weights = {\n",
        "    0: TermWeights(headline=1, short_description=1),\n",
        "    1: TermWeights(headline=3, short_description=1),\n",
        "    2: TermWeights(headline=1, short_description=3)\n",
        "}\n",
        "\n",
        "permutations = [\n",
        "    term_funcs,\n",
        "    [True, False],  # stem\n",
        "    sim_funcs,\n",
        "    region_weights\n",
        "]\n",
        "\n",
        "# permutations = [\n",
        "#     ('tfidf', False, 'cosine', 0)\n",
        "# ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QykHZjr5jGPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, categories = read_docs(file)\n",
        "split = int(len(docs) * 0.9)\n",
        "\n",
        "training_docs = docs[:split]\n",
        "testing_docs = docs[split:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIhL1hWjjr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "e1335afd-c4d7-4fb7-b4d1-38644365a01e"
      },
      "source": [
        "print('category', 'accuracy', 'precision', 'recall', 'F1_Score', sep='\\t')\n",
        "\n",
        "for term, stem, sim, weight in itertools.product(*permutations):\n",
        "\n",
        "    processed_traindocs, processed_devdocs = process_docs(training_docs, testing_docs, stem)\n",
        "    doc_freqs_train = compute_doc_freqs(processed_traindocs)\n",
        "    doc_freqs_test = compute_doc_freqs(processed_devdocs)\n",
        "    N_train = len(processed_traindocs)\n",
        "    N_test = len(processed_devdocs)\n",
        "    metrics = []\n",
        "\n",
        "    # create Vprofile\n",
        "    vprofile = defaultdict(lambda: defaultdict(float))\n",
        "    train_vector_sense = defaultdict(list)\n",
        "\n",
        "    for train_doc in processed_traindocs:\n",
        "        train_vector = term_funcs[term](train_doc, doc_freqs_train, region_weights[weight], N_train)\n",
        "        train_vector_sense[train_doc.category].append(train_vector)\n",
        "\n",
        "    # do centriod\n",
        "    for category, vecs in train_vector_sense.items():\n",
        "        for doc in vecs:\n",
        "            for word in doc:\n",
        "                vprofile[category][word] += doc[word]\n",
        "\n",
        "    for category, vec in vprofile.items():\n",
        "        for word in vec:\n",
        "            vec[word] /= len(vec)\n",
        "\n",
        "    # calculate correctness\n",
        "    correct_count = 0\n",
        "    sims = defaultdict(float)\n",
        "    tp = defaultdict(float)\n",
        "    fn = defaultdict(float)\n",
        "    fp = defaultdict(float)\n",
        "\n",
        "    for test_doc in processed_devdocs:\n",
        "        test_vector = term_funcs[term](test_doc, doc_freqs_test, region_weights[weight], N_test)\n",
        "        for category, vec in vprofile.items():\n",
        "            sims[category] = sim_funcs[sim](test_vector, vec)\n",
        "\n",
        "        predict = max(sims, key=lambda key: sims[key])\n",
        "        if predict == test_doc.category:\n",
        "            correct_count += 1\n",
        "            tp[predict] += 1\n",
        "        else:\n",
        "            fp[predict] += 1\n",
        "            fn[test_doc.category] += 1\n",
        "\n",
        "        metrics.append([\n",
        "            test_doc.doc_id, predict, test_doc.category\n",
        "        ])\n",
        "\n",
        "    total_acc = correct_count / len(processed_devdocs)\n",
        "    accuracy = defaultdict(float)\n",
        "    precison = defaultdict(float)\n",
        "    recall = defaultdict(float)\n",
        "    f1_score = defaultdict(float)\n",
        "\n",
        "    for category in categories:\n",
        "        accuracy[category] = tp[category] / (tp[category] + fp[category]) if (tp[category] + fp[category]) != 0 else 0\n",
        "        precison[category] = tp[category] / (tp[category] + fn[category]) if (tp[category] + fn[category]) != 0 else 0\n",
        "        recall[category] = (tp[category] + N_test - fn[category]) / (tp[category] + fp[category] + N_test)\n",
        "        f1_score[category] = 2 * precison[category] * recall[category] / (precison[category] + recall[category])\n",
        "        print(category, accuracy[category], precison[category], recall[category], f1_score[category], sep='\\t')\n",
        "\n",
        "    print(term, stem, sim, weight, str(total_acc))\n",
        "\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category\taccuracy\tprecision\trecall\tF1_Score\n",
            "TASTE\t0.0\t0\t0.974291812184711\t0.0\n",
            "HEALTHY LIVING\t0.0\t0\t0.9345803089521683\t0.0\n",
            "WELLNESS\t0.8335396039603961\t0.29741664826672554\t0.8409823979356741\t0.4394279262262694\n",
            "STYLE & BEAUTY\t0.9157846295444086\t0.5243741765480896\t0.9106878116716833\t0.6655338588267137\n",
            "LATINO VOICES\t0.0\t0\t0.9994029256642452\t0.0\n",
            "WORLD NEWS\t0.0\t0\t0.9950953678474115\t0.0\n",
            "QUEER VOICES\t0.8167539267015707\t0.5492957746478874\t0.9919613355032796\t0.707059408345678\n",
            "THE WORLDPOST\t0.0\t0\t0.9960823208529631\t0.0\n",
            "COMEDY\t0.4336569579288026\t0.3952802359882006\t0.9813679823486149\t0.563564987027232\n",
            "COLLEGE\t0.0\t0\t0.9928327813751174\t0.0\n",
            "ENVIRONMENT\t0.34274193548387094\t0.2750809061488673\t0.9809678371200944\t0.42967364600134333\n",
            "STYLE\t0.0\t0\t0.9762807426849421\t0.0\n",
            "GOOD NEWS\t0.0\t0\t0.9905804606204074\t0.0\n",
            "HOME & LIVING\t0.6114221724524076\t0.5465465465465466\t0.9618666285332952\t0.6970303531529634\n",
            "MEDIA\t0.0\t0\t0.9913627165490351\t0.0\n",
            "CULTURE & ARTS\t0.3624454148471616\t0.4256410256410256\t0.9873000246123554\t0.5948378313674906\n",
            "WOMEN\t0.0\t0\t0.9816724500268804\t0.0\n",
            "PARENTING\t0.7058823529411765\t0.24883359253499224\t0.920591351247231\t0.39177213451015774\n",
            "GREEN\t0.0\t0\t0.9915584736140594\t0.0\n",
            "ENTERTAINMENT\t0.2152619589977221\t0.5510204081632653\t0.9597882083571837\t0.7001057374656644\n",
            "TECH\t0.5359712230215827\t0.5580524344569289\t0.9878707523079945\t0.7132096639392778\n",
            "FIFTY\t0.0\t0\t0.9331475029036005\t0.0\n",
            "RELIGION\t0.0\t0\t0.9958353991075856\t0.0\n",
            "IMPACT\t0.13028169014084506\t0.357487922705314\t0.9696426842258158\t0.5223834747536343\n",
            "CRIME\t0.27099236641221375\t0.6120689655172413\t0.988401808531551\t0.7559901527384387\n",
            "ARTS\t0.0\t0\t0.9880952380952381\t0.0\n",
            "WEIRD NEWS\t0.0\t0\t0.9918032786885246\t0.0\n",
            "DIVORCE\t0.9095940959409594\t0.5834319526627219\t0.9805604033352724\t0.7315768119040318\n",
            "TRAVEL\t0.8394957983193277\t0.4921182266009852\t0.9425643918029705\t0.6466281963704804\n",
            "WEDDINGS\t0.8560126582278481\t0.5372393247269116\t0.9731151655565209\t0.6922821599676172\n",
            "SCIENCE\t0.3248081841432225\t0.5852534562211982\t0.9827123113737364\t0.7336075679570334\n",
            "FOOD & DRINK\t0.6235864297253635\t0.4080338266384778\t0.961700072446269\t0.5729669987739333\n",
            "PARENTS\t0.0\t0\t0.9734890709058305\t0.0\n",
            "BLACK VOICES\t0.30857142857142855\t0.2523364485981308\t0.9861309905730221\t0.4018463209340211\n",
            "WORLDPOST\t0.0\t0\t0.9900433753943217\t0.0\n",
            "BUSINESS\t0.25963488843813387\t0.3256997455470738\t0.9693862675543029\t0.48757975529853775\n",
            "POLITICS\t0.0\t0\t0.9908736618815056\t0.0\n",
            "ARTS & CULTURE\t0.0\t0\t0.9941103687206138\t0.0\n",
            "MONEY\t0.7307032590051458\t0.41929133858267714\t0.963858919154289\t0.584372802819387\n",
            "EDUCATION\t0.0\t0\t0.9920972043860515\t0.0\n",
            "SPORTS\t0.23052959501557632\t0.6981132075471698\t0.9863282207085804\t0.8175633136335019\n",
            "tfidf False cosine 0 0.42477347406153537\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahlBRPt5FGky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sep_docs(file):\n",
        "    docs = defaultdict(list)\n",
        "    for d in file:\n",
        "        docs[d.category].append(d)\n",
        "    return docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp2JDpCSE_vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, categories = read_docs(file)\n",
        "sep_doc = sep_docs(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfnmZ76oFZ8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d697190b-d454-49e3-a000-11eed0307166"
      },
      "source": [
        "selected_categories = ['QUEER VOICES', 'SPORTS', 'COMEDY', 'HEALTHY LIVING', 'TRAVEL']\n",
        "training_docs = []\n",
        "testing_docs = []\n",
        "for c in selected_categories:\n",
        "    print(c, len(sep_doc[c]))\n",
        "    split = int(len(sep_doc[c]) * 0.9)\n",
        "    training_docs += (sep_doc[c][:split])\n",
        "    testing_docs += (sep_doc[c][split:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUEER VOICES 6314\n",
            "SPORTS 4884\n",
            "COMEDY 5175\n",
            "HEALTHY LIVING 6694\n",
            "TRAVEL 9887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXLiEeaFJcJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3077811-00c6-4d00-93bd-92f3b80cc931"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_docs)\n",
        "random.shuffle(testing_docs)\n",
        "print(\"Training data\", len(training_docs))\n",
        "print(\"Testing data\",len(testing_docs))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data 29656\n",
            "Testing data 3298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xbe0KP6FkBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e5a6ec8c-5b0e-423a-cbdc-eb46c8c02720"
      },
      "source": [
        "print('category', 'accuracy', 'precision', 'recall', 'F1_Score', sep='\\t')\n",
        "\n",
        "for term, stem, sim, weight in itertools.product(*permutations):\n",
        "\n",
        "    processed_traindocs, processed_devdocs = process_docs(training_docs, testing_docs, stem)\n",
        "    doc_freqs_train = compute_doc_freqs(processed_traindocs)\n",
        "    doc_freqs_test = compute_doc_freqs(processed_devdocs)\n",
        "    N_train = len(processed_traindocs)\n",
        "    N_test = len(processed_devdocs)\n",
        "    metrics = []\n",
        "\n",
        "    # create Vprofile\n",
        "    vprofile = defaultdict(lambda: defaultdict(float))\n",
        "    train_vector_sense = defaultdict(list)\n",
        "\n",
        "    for train_doc in processed_traindocs:\n",
        "        train_vector = term_funcs[term](train_doc, doc_freqs_train, region_weights[weight], N_train)\n",
        "        train_vector_sense[train_doc.category].append(train_vector)\n",
        "\n",
        "    # do centriod\n",
        "    for category, vecs in train_vector_sense.items():\n",
        "        for doc in vecs:\n",
        "            for word in doc:\n",
        "                vprofile[category][word] += doc[word]\n",
        "\n",
        "    for category, vec in vprofile.items():\n",
        "        for word in vec:\n",
        "            vec[word] /= len(vec)\n",
        "\n",
        "    # calculate correctness\n",
        "    correct_count = 0\n",
        "    sims = defaultdict(float)\n",
        "    tp = defaultdict(float)\n",
        "    fn = defaultdict(float)\n",
        "    fp = defaultdict(float)\n",
        "\n",
        "    for test_doc in processed_devdocs:\n",
        "        test_vector = term_funcs[term](test_doc, doc_freqs_test, region_weights[weight], N_test)\n",
        "        for category, vec in vprofile.items():\n",
        "            sims[category] = sim_funcs[sim](test_vector, vec)\n",
        "\n",
        "        predict = max(sims, key=lambda key: sims[key])\n",
        "        if predict == test_doc.category:\n",
        "            correct_count += 1\n",
        "            tp[predict] += 1\n",
        "        else:\n",
        "            fp[predict] += 1\n",
        "            fn[test_doc.category] += 1\n",
        "\n",
        "        metrics.append([\n",
        "            test_doc.doc_id, predict, test_doc.category\n",
        "        ])\n",
        "\n",
        "    total_acc = correct_count / len(processed_devdocs)\n",
        "    accuracy = defaultdict(float)\n",
        "    precison = defaultdict(float)\n",
        "    recall = defaultdict(float)\n",
        "    f1_score = defaultdict(float)\n",
        "\n",
        "    for category in selected_categories:\n",
        "        accuracy[category] = tp[category] / (tp[category] + fp[category]) if (tp[category] + fp[category]) != 0 else 0\n",
        "        precison[category] = tp[category] / (tp[category] + fn[category]) if (tp[category] + fn[category]) != 0 else 0\n",
        "        recall[category] = (tp[category] + N_test - fn[category]) / (tp[category] + fp[category] + N_test)\n",
        "        f1_score[category] = 2 * precison[category] * recall[category] / (precison[category] + recall[category])\n",
        "        print(category, accuracy[category], precison[category], recall[category], f1_score[category], sep='\\t')\n",
        "\n",
        "    print(term, stem, sim, weight, str(total_acc))\n",
        "\n",
        "    break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category\taccuracy\tprecision\trecall\tF1_Score\n",
            "QUEER VOICES\t0.8155172413793104\t0.7484177215189873\t0.9314079422382672\t0.8299458985231759\n",
            "SPORTS\t0.8107569721115537\t0.8323108384458078\t0.953421052631579\t0.8887590345143084\n",
            "COMEDY\t0.7757731958762887\t0.581081081081081\t0.9175257731958762\t0.7115366737938087\n",
            "HEALTHY LIVING\t0.7395411605937922\t0.817910447761194\t0.9220103986135182\t0.8668462586004912\n",
            "TRAVEL\t0.766329346826127\t0.8422649140546006\t0.9064994298745724\t0.873202460976954\n",
            "tfidf False cosine 0 0.7768344451182535\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}