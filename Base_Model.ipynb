{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Base_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPcgXEWI8ZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c95764ca-308e-4d9d-ca29-039872b61e27"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, NamedTuple\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm, svd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HecDdqTgiIWj",
        "colab_type": "text"
      },
      "source": [
        "File IO and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CiWULfI8Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Document(NamedTuple):\n",
        "    doc_id: str\n",
        "    category: str\n",
        "    link: str\n",
        "    date: str\n",
        "    headline: List[str]\n",
        "    short_description: List[str]\n",
        "\n",
        "    def sections(self):\n",
        "        return [self.headline, self.short_description]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"doc_id: {self.doc_id}\\n\" +\n",
        "            f\"  category: {self.category}\\n\" +\n",
        "            f\"  link: {self.link}\\n\" +\n",
        "            f\"  date: {self.date}\" +\n",
        "            f\"  headline: {self.headline}\" +\n",
        "            f\"  short_description: {self.short_description}\")\n",
        "\n",
        "\n",
        "def read_stopwords(file):\n",
        "    with open(file) as f:\n",
        "        return set([x.strip() for x in f.readlines()])\n",
        "\n",
        "stopwords = read_stopwords('common_words')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "def read_docs(file):\n",
        "    '''\n",
        "    Reads the corpus into a list of Documents\n",
        "    '''\n",
        "    docs = []  # empty 0 index\n",
        "    categories = set()\n",
        "    with open(file) as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            json_dict = json.loads(line)\n",
        "            doc_id = i\n",
        "            category = json_dict['category']\n",
        "            categories.add(category)\n",
        "            link = json_dict['link']\n",
        "            date = json_dict['date']\n",
        "            headline = []\n",
        "            short_description = []\n",
        "\n",
        "            ws = word_tokenize(json_dict['headline'])\n",
        "            for word in ws:\n",
        "                headline.append(word.lower())\n",
        "\n",
        "            ws = word_tokenize(json_dict['short_description'])\n",
        "            for word in ws:\n",
        "                short_description.append(word.lower())\n",
        "            docs.append(Document(doc_id, category, link, date, headline, short_description))\n",
        "            i += 1\n",
        "\n",
        "    return docs, categories\n",
        "\n",
        "\n",
        "def stem_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[stemmer.stem(word) for word in sec]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def stem_docs(docs: List[Document]):\n",
        "    return [stem_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def remove_stopwords_doc(doc: Document):\n",
        "    return Document(doc.doc_id, doc.category, doc.link, doc.date, *[[word for word in sec if word not in stopwords]\n",
        "        for sec in doc.sections()])\n",
        "\n",
        "\n",
        "def remove_stopwords(docs: List[Document]):\n",
        "    return [remove_stopwords_doc(doc) for doc in docs]\n",
        "\n",
        "\n",
        "def writelines(filename, data):\n",
        "    with open(filename, 'w') as fout:\n",
        "        for d in data:\n",
        "            print(d, file=fout)\n",
        "\n",
        "\n",
        "def process_docs(training_docs, testing_docs, stem):\n",
        "    processed_traindocs = training_docs\n",
        "    processed_devdocs = testing_docs\n",
        "\n",
        "    if stem:\n",
        "        processed_traindocs = stem_docs(processed_traindocs)\n",
        "        processed_devdocs = stem_docs(processed_devdocs)\n",
        "    return processed_traindocs, processed_devdocs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hagHDXMiUAi",
        "colab_type": "text"
      },
      "source": [
        "Position weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4AVh-DLtOnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TermWeights(NamedTuple):\n",
        "    headline: float\n",
        "    short_description: float"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0DnoPX1ihT1",
        "colab_type": "text"
      },
      "source": [
        "Term-Document Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGIxU6bcieMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_doc_freqs(docs: List[Document]):\n",
        "    '''\n",
        "    Computes document frequency, i.e. how many documents contain a specific word\n",
        "    '''\n",
        "    freq = Counter()\n",
        "    for doc in docs:\n",
        "        words = set()\n",
        "        for sec in doc.sections():\n",
        "            for word in sec:\n",
        "                words.add(word)\n",
        "        for word in words:\n",
        "            freq[word] += 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def compute_tf(doc: Document, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] += weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] += weights.short_description\n",
        "    return dict(vec)  # convert back to a regular dict\n",
        "\n",
        "\n",
        "def compute_tfidf(doc: Document, doc_freqs, weights, N):\n",
        "    tf = compute_tf(doc, doc_freqs, weights, N)\n",
        "    vec = defaultdict(float)\n",
        "    for k, v in tf.items():\n",
        "        if doc_freqs[k] == 0:\n",
        "            continue\n",
        "        vec[k] = v * np.log(float(N) / (doc_freqs[k]))\n",
        "\n",
        "    return dict(vec)  # TODO: implement\n",
        "\n",
        "def compute_boolean(doc, doc_freqs, weights, N):\n",
        "    vec = defaultdict(float)\n",
        "    for word in doc.headline:\n",
        "        vec[word] = 1 * weights.headline\n",
        "    for word in doc.short_description:\n",
        "        vec[word] = 1 * weights.short_description\n",
        "    return dict(vec)  # TODO: implement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYUCh1RsilDY",
        "colab_type": "text"
      },
      "source": [
        "Vector Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7XiYHJ5ims3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dictdot(x: Dict[str, float], y: Dict[str, float]):\n",
        "    '''\n",
        "    Computes the dot product of vectors x and y, represented as sparse dictionaries.\n",
        "    '''\n",
        "    keys = list(x.keys()) if len(x) < len(y) else list(y.keys())\n",
        "    return sum(x.get(key, 0) * y.get(key, 0) for key in keys)\n",
        "\n",
        "\n",
        "def cosine_sim(x, y):\n",
        "    '''\n",
        "    Computes the cosine similarity between two sparse term vectors represented as dictionaries.\n",
        "    '''\n",
        "    num = dictdot(x, y)\n",
        "    if num == 0:\n",
        "        return 0\n",
        "    return num / (norm(list(x.values())) * norm(list(y.values())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsjiM1Yi_OK",
        "colab_type": "text"
      },
      "source": [
        "Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoQrZvli-nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = 'News_Category_Dataset.json'\n",
        "\n",
        "term_funcs = {\n",
        "    'tfidf': compute_tfidf,\n",
        "    'tf': compute_tf,\n",
        "    'boolean': compute_boolean\n",
        "}\n",
        "\n",
        "sim_funcs = {\n",
        "    'cosine': cosine_sim\n",
        "}\n",
        "\n",
        "region_weights = {\n",
        "    0: TermWeights(headline=1, short_description=1),\n",
        "    1: TermWeights(headline=3, short_description=1),\n",
        "    2: TermWeights(headline=1, short_description=3)\n",
        "}\n",
        "\n",
        "permutations = [\n",
        "    term_funcs,\n",
        "    [False, True],  # stem\n",
        "    sim_funcs,\n",
        "    region_weights\n",
        "]\n",
        "\n",
        "# permutations = [\n",
        "#     ('tfidf', False, 'cosine', 0)\n",
        "# ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QykHZjr5jGPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, categories = read_docs(file)\n",
        "split = int(len(docs) * 0.9)\n",
        "\n",
        "training_docs = docs[:split]\n",
        "testing_docs = docs[split:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIhL1hWjjr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1335afd-c4d7-4fb7-b4d1-38644365a01e"
      },
      "source": [
        "print('category', 'accuracy', 'precision', 'recall', 'F1_Score', sep='\\t')\n",
        "\n",
        "for term, stem, sim, weight in itertools.product(*permutations):\n",
        "\n",
        "    processed_traindocs, processed_devdocs = process_docs(training_docs, testing_docs, stem)\n",
        "    doc_freqs_train = compute_doc_freqs(processed_traindocs)\n",
        "    doc_freqs_test = compute_doc_freqs(processed_devdocs)\n",
        "    N_train = len(processed_traindocs)\n",
        "    N_test = len(processed_devdocs)\n",
        "    metrics = []\n",
        "\n",
        "    # create Vprofile\n",
        "    vprofile = defaultdict(lambda: defaultdict(float))\n",
        "    train_vector_sense = defaultdict(list)\n",
        "\n",
        "    for train_doc in processed_traindocs:\n",
        "        train_vector = term_funcs[term](train_doc, doc_freqs_train, region_weights[weight], N_train)\n",
        "        train_vector_sense[train_doc.category].append(train_vector)\n",
        "\n",
        "    # do centriod\n",
        "    for category, vecs in train_vector_sense.items():\n",
        "        for doc in vecs:\n",
        "            for word in doc:\n",
        "                vprofile[category][word] += doc[word]\n",
        "\n",
        "    for category, vec in vprofile.items():\n",
        "        for word in vec:\n",
        "            vec[word] /= len(vec)\n",
        "\n",
        "    # calculate correctness\n",
        "    correct_count = 0\n",
        "    sims = defaultdict(float)\n",
        "    tp = defaultdict(float)\n",
        "    fn = defaultdict(float)\n",
        "    fp = defaultdict(float)\n",
        "\n",
        "    for test_doc in processed_devdocs:\n",
        "        test_vector = term_funcs[term](test_doc, doc_freqs_test, region_weights[weight], N_test)\n",
        "        for category, vec in vprofile.items():\n",
        "            sims[category] = sim_funcs[sim](test_vector, vec)\n",
        "\n",
        "        predict = max(sims, key=lambda key: sims[key])\n",
        "        if predict == test_doc.category:\n",
        "            correct_count += 1\n",
        "            tp[predict] += 1\n",
        "        else:\n",
        "            fp[predict] += 1\n",
        "            fn[test_doc.category] += 1\n",
        "\n",
        "        metrics.append([\n",
        "            test_doc.doc_id, predict, test_doc.category\n",
        "        ])\n",
        "\n",
        "    total_acc = correct_count / len(processed_devdocs)\n",
        "    accuracy = defaultdict(float)\n",
        "    precison = defaultdict(float)\n",
        "    recall = defaultdict(float)\n",
        "    f1_score = defaultdict(float)\n",
        "\n",
        "    for category in categories:\n",
        "        accuracy[category] = tp[category] / (tp[category] + fp[category]) if (tp[category] + fp[category]) != 0 else 0\n",
        "        precison[category] = tp[category] / (tp[category] + fn[category]) if (tp[category] + fn[category]) != 0 else 0\n",
        "        recall[category] = (tp[category] + N_test - fn[category]) / (tp[category] + fp[category] + N_test)\n",
        "        f1_score[category] = 2 * precison[category] * recall[category] / (precison[category] + recall[category])\n",
        "        print(category, accuracy[category], precison[category], recall[category], f1_score[category], sep='\\t')\n",
        "\n",
        "    print(term, stem, sim, weight, str(total_acc))\n",
        "\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category\taccuracy\tprecision\trecall\tF1_Score\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}